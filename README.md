# Data waherehouse and Analytics Project**

ğŸ“Œ Overview

This project demonstrates a complete data warehousing and analytics solution, built to showcase modern data engineering practices. It simulates a real-world scenario: ingesting data from multiple source systems (ERP and CRM), transforming it into a structured data warehouse, and delivering actionable insights via a BI dashboard.

### Objective
The goal is to Develop a modern warehouse using SQL Server,illustrate proficiency in data modeling, ETL/ELT pipelines, cloud data platforms, and business intelligence.

ğŸ—ï¸ Architecture

Â· Source Systems: Simulated transactional data (e.g., sales, customers, products) loaded into a staging area.
Â· Ingestion: Python scripts and Apache Airflow DAGs to extract and load raw data.
Â· Warehouse: Snowflake (or BigQuery/Redshift) â€“ raw, staging, and mart layers using a star schema.
Â· Transformation: dbt (Data Build Tool) for modular, tested SQL transformations.
Â· BI: Interactive dashboards built with Power BI (or Tableau/Looker).

ğŸ› ï¸ Tech Stack

Component Technology
Database Snowflake
Orchestration Apache Airflow
Transformation dbt
Version Control Git & GitHub
BI / Visualization Power BI
Language SQL, Python
Infrastructure Docker (optional)

ğŸ“Š Dataset

The project uses a publicly available dataset (e.g., TPC-H, AdventureWorks, or a custom-generated e-commerce dataset) modified to include realistic dimensions and facts. The data includes:

Â· Customers (demographics, segments)
Â· Products (categories, pricing)
Â· Sales Orders (transactions, quantities, dates)
Â· Inventory (stock levels, movements)

ğŸ“ Repository Structure

```
.
â”œâ”€â”€ airflow/ # Airflow DAGs for orchestration
â”œâ”€â”€ dbt/ # dbt models, macros, tests, and docs
â”‚ â”œâ”€â”€ models/
â”‚ â”‚ â”œâ”€â”€ staging/ # Clean and standardize raw data
â”‚ â”‚ â”œâ”€â”€ marts/ # Fact and dimension tables
â”‚ â”‚ â””â”€â”€ intermediate/ # Business-level transformations
â”‚ â”œâ”€â”€ tests/ # Custom data quality tests
â”‚ â””â”€â”€ dbt_project.yml
â”œâ”€â”€ sql/ # Raw DDL/DML scripts (optional)
â”œâ”€â”€ notebooks/ # Jupyter notebooks for exploration
â”œâ”€â”€ dashboard/ # Power BI/Tableau files
â”œâ”€â”€ docker-compose.yml # For local Airflow setup
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md
```

ğŸš€ How to Run the Project Locally

1. Clone the repository
      git clone https://github.com/yourusername/data-warehouse-portfolio.git
2. Set up the data warehouse
   Â· Create a Snowflake (or your chosen DB) account and configure credentials.
   Â· Run the initial setup scripts in sql/ to create databases and schemas.
3. Install dependencies
      pip install -r requirements.txt
4. Configure dbt
   Â· Update dbt/profiles.yml with your database connection details.
   Â· Run dbt debug to verify connection.
5. Run dbt transformations
      cd dbt && dbt run
6. Orchestrate with Airflow (optional)
   Â· Use Docker Compose: docker-compose up -d
   Â· Access Airflow UI at http://localhost:8080 and trigger the DAG.
7. Explore the BI dashboard
   Â· Open the Power BI/Tableau file in dashboard/ and connect to your warehouse.

ğŸ” Key Features

Â· Modular dbt models with clear staging, intermediate, and mart layers.
Â· Data quality tests (not-null, uniqueness, referential integrity) via dbt.
Â· Incremental loading for large fact tables.
Â· Airflow DAG to automate the entire pipeline.
Â· Interactive dashboards showing KPIs: revenue trends, customer segmentation, inventory turnover.

ğŸ“ˆ What I Learned

Â· Designing a star schema for analytical queries.
Â· Implementing ELT pipelines with dbt and Airflow.
Â· Writing efficient SQL for transformations.
Â· Building a production-like environment for data engineering projects.

ğŸ“œ License
This Project is licensed under MIT License- see the License [MIT License](LICENSE). file  for Detail

ğŸ“§ Contact
Email: LukmanAbdulkadri1@gmail.com/
Feel free to connect with me on LinkedIn or check out my other projects on GitHub.

---

This project is for educational purposes and part of my data engineering portfolio.
